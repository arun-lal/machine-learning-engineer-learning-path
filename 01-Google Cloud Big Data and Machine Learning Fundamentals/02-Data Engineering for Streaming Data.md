# Data Engineering for Streaming Data

## Streaming Data vs Batch Processing
**Batch processing** is when the processing and analysis happens on a set of stored data. An example is Payroll and billing systems that have to be processed on either a weekly or monthly basis.

**Streaming data** is a flow of data records generated by various data sources. The processing of streaming data happens as the data flows through a system.
This results in the analysis and reporting of events as they happen. An example would be fraud detection or intrusion detection.

Streaming data processing means that the data is analyzed in near real-time and that actions will be taken on the data as quickly as possible.
Modern data processing has progressed from legacy batch processing of data toward working with real-time data streams. An example of this is streaming music and movies.
## Big data challenges
Building scalable and reliable pipelines is a core responsibility of data engineers. However, in modern organizations, data engineers and data scientists are facing four major challenges. These are collectively known as the 4 Vs. They are **variety**, **volume**, **velocity**, and **veracity**.
1. **Variety:** data could come in from a variety of different sources and in various formats.
2. **Volume:** handle not only an arbitrary variety of input sources, but a volume of data that varies from gigabytes to petabytes.
3. **Velocity:** Data often needs to be processed in near-real time, as soon as it reaches the system.
4. **Veracity:** it refers to the data quality.

## Message-oriented Architecture
One of the early stages in a data pipeline is data ingestion, which is where large amounts of streaming data are received. Data, however, may not always come from a single, structured database. Instead, the data might stream from a thousand, or even a million, different events that are all happening asynchronously.
This present new challenges to data ingestion, which can be summarized in four points: 
1. The first is that data can be streamed from many different methods and devices, many of which might not talk to each other and might be sending bad or delayed data.
2. It can be hard to distribute event messages to the right subscribers.
3. Data can arrive quickly and at high volumes.
4. Ensuring services are reliable, secure, and perform as expected.

### Pub/Sub
- Google Cloud has a tool to handle distributed message-oriented architectures at scale, and that is Pub/Sub (Publisher/Subscriber).
- Pub/Sub is a distributed messaging service that can receive messages from a variety of device streams such as gaming events, IoT devices, and application streams.
- It ensures at-least-once delivery of received messages to subscribing applications, with no provisioning required.
- Pub/Sub’s APIs are open, the service is global by default, and it offers end-to-end encryption.

#### End-to-end architecture using Pub/Sub.
- Upstream source data comes in from devices all over the globe and is ingested into Pub/Sub, which is the first point of contact within the system.
- Pub/Sub reads, stores, broadcasts to any subscribers of this data topic that new messages are available.
- As a subscriber of Pub/Sub, Dataflow can ingest and transform those messages in an elastic streaming pipeline and output the results into an analytics data warehouse like BigQuery.
- Finally, you can connect a data visualization tool, like Looker, to visualize and monitor the results of a pipeline, or an AI or ML tool such as Vertex AI to explore the data to uncover business insights or help with predictions.

A central element of Pub/Sub is the topic. You can think of a topic like a radio antenna. Whether your radio is playing music or it’s turned off, the antenna itself is always there. If music is being broadcast on a frequency that nobody’s listening to, the stream of music still exists.

Similarly, a publisher can send data to a topic that has no subscriber to receive it. Or a subscriber can be waiting for data from a topic that isn’t getting data sent to it, like listening to static from a bad radio frequency. Or you could have a fully operational pipeline where the publisher is sending data to a topic that an application is subscribed to. That means there can be zero, one, or more publishers, and zero, one or more subscribers related to a topic.
And they’re completely decoupled, so they’re free to break without affecting their counterparts.

Pub/Sub is a good solution to buffer changes for lightly coupled architectures, like this one, that have many different publishers and subscribers.
Pub/Sub supports many different inputs and outputs, and you can even publish a Pub/Sub event from one topic to another.

## Designing streaming pipelines with Apache Beam
 After messages have been captured from the streaming input sources you need a way to pipe that data into a data warehouse for analysis.
This is where Dataflow comes in.
Dataflow creates a pipeline to process both streaming data and batch data.
“Process” in this case refers to the steps to extract, transform, and load data, or ETL.
When building a data pipeline, data engineers often encounter challenges related to coding the pipeline design and implementing and serving the pipeline at scale.
During the pipeline design phase, there are a few questions to consider: 
- Will the pipeline code be compatible with both batch and streaming data, or will it need to be refactored?
- Will the pipeline code software development kit, or SDK, being used have all the transformations, mid-flight aggregations and windowing and be able to handle late data?
- Are there existing templates or solutions that should be referenced?

### Apache Beam
A popular solution for pipeline design is Apache Beam.
It’s an open source, unified programming model to define and execute data processing pipelines, including ETL, batch, and stream processing.
Apache Beam is unified, which means it uses a single programming model for both batch and streaming data.
It’s portable, which means it can work on multiple execution environments, like Dataflow and Apache Spark, among others.
And it’s extensible, which means it allows you to write and share your own connectors and transformation libraries.
Apache Beam provides pipeline templates, so you don’t need to build a pipeline from nothing.
And it can write pipelines in Java, Python, or Go.
The Apache Beam software development kit, or SDK, is a collection of software development tools in one installable package.
It provides a variety of libraries for transformations and data connectors to sources and sinks.
Apache Beam creates a model representation from your code that is portable across many runners.
Runners pass off your model for execution on a variety of different possible engines, with Dataflow being a popular choice.

## Implementing streaming pipelines on Cloud Dataflow
Apache Beam can be used to create data processing pipelines. The next step is to identify an execution engine to implement those pipelines.
When choosing an execution engine for your pipeline code, it might be helpful to consider the following questions:
- How much maintenance overhead is involved?
- Is the infrastructure reliable?
- How is the pipeline scaling handled?
- How can the pipeline be monitored?
- Is the pipeline locked in to a specific service provider?

### Dataflow
Dataflow is a fully managed service for executing Apache Beam pipelines within the Google Cloud ecosystem.
Dataflow handles much of the complexity relating to infrastructure setup and maintenance and is built on Google’s infrastructure.
This allows for reliable auto scaling to meet data pipeline demands.
Dataflow is serverless and NoOps, which means No Operations.
But what does that mean exactly?
A NoOps environment is one that doesn't require management from an operations team, because maintenance, monitoring, and scaling are automated.
Serverless computing is a cloud computing execution model.
This is when Google Cloud, for example, manages infrastructure tasks on behalf of the users.
This includes tasks like resource provisioning, performance tuning, and ensuring pipeline reliability.
Dataflow means that you can spend more time analyzing the insights from your datasets and less time provisioning resources to ensure that your pipeline will successfully complete its next cycles.
It’s designed to be low maintenance.
Let’s explore the tasks Dataflow performs when a job is received.
It starts by optimizing a pipeline model's execution graph to remove any inefficiencies.
Next, it schedules out distributed work to new workers and scales as needed.
After that, it auto-heals any worker faults.
From there, it automatically rebalances efforts to most efficiently use its workers.
And finally, it outputs data to produce a result.
BigQuery is one of many options that data can be outputted to.
You’ll get some more practice using BigQuery later in this course.
So by design, you don't need to monitor all of the compute and storage resources that Dataflow manages, to fit the demand of a streaming data pipeline.
Even experienced Java or Python developers will benefit from using Dataflow templates, which cover common use cases across Google Cloud products.
The list of templates is continuously growing.
They can be broken down into three categories: streaming templates, batch templates, and utility templates.
Streaming templates are for processing continuous, or real-time, data.
For example: Pub/Sub to BigQuery Pub/Sub to Cloud Storage Datastream to BigQuery Pub/Sub to MongoDB Batch templates are for processing bulk data, or batch load data.
For example: BigQuery to Cloud Storage Bigtable to Cloud Storage Cloud Storage to BigQuery Cloud
Spanner to Cloud Storage Finally, utility templates address activities related to bulk compression, deletion, and conversion.

## Visualization with Looker

## Visualization with Data Studio
